---
title: "Stat 651 Project"
author: "Talmage Hilton"
date: "2025-04-08"
output: html_document
---

```{r setup, include=FALSE, waning=FALSE}
library(readxl)
library(tidyverse)
library(dplyr)
library(MASS)
library(coda)
library(MCMCpack)
library(reshape2)
library(mvtnorm)
```

```{r}
# Read in data
ratdata <- read_excel("ratdata.xlsx")

# Pivot longer to get tidy data
rat_long <- ratdata %>%
  pivot_longer(cols = starts_with("rat"), names_to = "rat", values_to = "weight")
```



# EDA

```{r, warning=FALSE}
# Weights of each rat
ggplot(rat_long, aes(x = age, y = weight, group = rat, color = rat)) +
  geom_line() +
  geom_point() +
  labs(title = "Growth Curves for Each Rat", x = "Age (days)", y = "Weight (g)") +
  theme_minimal() +
  theme(legend.position = "none")


# Growth rate for each rat
rat_diff <- ratdata %>%
  dplyr::select(-age) %>%
  mutate(across(everything(), ~ c(NA, diff(.)))) %>%
  mutate(age = ratdata$age)

rat_diff_long <- rat_diff %>%
  pivot_longer(cols = -age, names_to = "rat", values_to = "growth_rate")

ggplot(rat_diff_long, aes(x = age, y = growth_rate, group = rat, color = rat)) +
  geom_line() +
  labs(title = "Growth Rate by Age", x = "Age (days)", y = "Δ Weight (g)") +
  theme_minimal() +
  theme(legend.position = "none")


# Boxplots at each age
ggplot(rat_long, aes(x = factor(age), y = weight)) +
  geom_boxplot() +
  labs(title = "Distribution of Rat Weights at Each Age", x = "Age (days)", y = "Weight (g)") +
  theme_minimal()
```

The growth curves show that the rats grow in a mostly linear fashion. Obviously there is some variability, but overall I'd say that it's pretty linear. The growth rates are not very consistent (lots of ups and downs), but overall they're all pretty similar. For the most part the growth rate slows down between days 15-22, then speed up between days 22-29, and then decrease again from days 29-36. However, there are a few rate that increase in growth rate during the last time period. There are some exceptions to the rule here, but overall I'd say that the Normal model for $Y_{ij}$ is fairly reasonable.


```{r}
# Justify the prior
library(MASS)       # For mvrnorm
library(ggplot2)    # For prettier plots
library(reshape2)   # For melt
library(gridExtra)  # For side-by-side plots

# Prior parameters
eta <- c(0, 0)
C <- diag(5, 2)
lambda_0 <- 0.1
nu_0 <- 0.1

# Sample from prior
set.seed(123)
n_samples <- 100

# Sample tau ~ Inv-Gamma
# Inv-Gamma(a, b) → 1 / rgamma(n, shape = a, rate = b)
tau <- 1 / rgamma(n_samples, shape = nu_0 / 2, rate = (nu_0 * lambda_0) / 2)

# For each tau, sample alpha_c and beta_c ~ N2(eta, tau * C)
alphas_betas <- t(sapply(tau, function(t) {
  mvrnorm(1, mu = eta, Sigma = t * C)
}))

# Organize samples
df <- data.frame(alpha_c = alphas_betas[,1],
                 beta_c = alphas_betas[,2],
                 tau = tau)

# Marginal density plots
p1 <- ggplot(df, aes(x = alpha_c)) + geom_density(fill = "skyblue") + ggtitle("Prior for alpha_c")
p2 <- ggplot(df, aes(x = beta_c)) + geom_density(fill = "orange") + ggtitle("Prior for beta_c")
p3 <- ggplot(df, aes(x = tau)) + geom_density(fill = "green") + ggtitle("Prior for tau")

# Contour plot of joint (alpha_c, beta_c)
p4 <- ggplot(df, aes(x = alpha_c, y = beta_c)) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon", color = "black") +
  scale_fill_viridis_c() + ggtitle("Joint Prior: alpha_c vs beta_c")

# Plot all together
grid.arrange(p1, p2, p3, p4, ncol = 2)
```





# Question 2

```{r}
# Nest the data by rat
rat_nested <- rat_long %>%
  group_by(rat) %>%
  nest()

# Fit regression to each nested dataset
rat_models <- rat_nested %>%
  mutate(model = map(data, ~lm(weight ~ age, data = .)))

# Extract coefficients from each model
rat_coefs <- rat_models %>%
  mutate(coefs = map(model, broom::tidy)) %>%
  unnest(coefs) %>%
  dplyr::select(rat, term, estimate) %>%
  pivot_wider(names_from = term, values_from = estimate) %>%
  rename(alpha_hat = `(Intercept)`, beta_hat = age)


# Check Normality of thetas

# Histogram of intercepts
ggplot(rat_coefs, aes(x = alpha_hat)) +
  geom_histogram(bins = 10, fill = "skyblue") +
  labs(title = "Histogram of Intercepts (alpha_i)", x = expression(hat(alpha)[i]))

# Histogram of slopes
ggplot(rat_coefs, aes(x = beta_hat)) +
  geom_histogram(bins = 10, fill = "lightgreen") +
  labs(title = "Histogram of Slopes (beta_i)", x = expression(hat(beta)[i]))

# QQ plots
qqnorm(rat_coefs$alpha_hat); qqline(rat_coefs$alpha_hat, col = "blue")
qqnorm(rat_coefs$beta_hat); qqline(rat_coefs$beta_hat, col = "blue")

```

I would say that the normality assumption is reasonable here. The distribution of intercepts is approximately normal. The distribution of slopes is a little right skewed, but I wouldn't say it's too egregious. The Q-Q plots also aren't too terrible.





# Gibbs Sampler

```{r}
# Reshape data
rats <- melt(ratdata, id.vars = c("age"))
colnames(rats) <- c("age",'id','weight')

# Priors
lambda0 <- 0.1
nu0 <- 0.1
eta <- matrix(c(0,0), nrow=2)
Sigma <- matrix(c(10, 0, 0, 10), nrow=2, ncol=2)
Sigma_inv <- solve(Sigma)
C <- matrix(c(5, 0, 0, 5), nrow=2, ncol=2)
C_inv <- solve(C)
V <- solve(30 * Sigma_inv + C_inv)


set.seed(123)

# Prepare data
age <- ratdata$age
n <- 30  # number of rats
t <- 5  # number of time points = 5

# Design matrix
rat_data <- lapply(unique(rats$id), function(j) {
    that_rat <- rats[rats$id == j, ]
    that_rat$weight          # Response variable
})
Xi <- cbind(rep(1, 5), c(8, 15, 22, 29, 36))  # Define X_i once
XtX <- t(Xi) %*% Xi
Y_matrix <- do.call(cbind, rat_data)  # Combine all Y_i into a matrix (5 x 30)

# Hyperparameters
n_iter <- 100000
burn_in <- 10000
thin <- 10

# Storage
alphas = matrix(nrow=n_iter,ncol=30)
betas = matrix(nrow=n_iter,ncol=30)
mu_cs = matrix(nrow=n_iter,ncol=2)
taus = matrix(nrow=n_iter,ncol=1)
sses = rep(0,n_iter)

# Initialize
alpha = rep(0,30)
beta = rep(0,30)
mu_c = rep(0,2)
tau = 1

# Gibbs sampler
for (i in 1:n_iter) {
  
  # --- 1. Sample theta_i | Y_i, mu, tau ---
  # Compute D_i_inv for all groups (same for all 30 rats)
  Di_inv <- (1/tau) * XtX + Sigma_inv
  Di <- solve(Di_inv)
  
  # Compute means (2 × 30 matrix)
  means <- Di %*% ((1/tau) * (t(Xi) %*% Y_matrix) + Sigma_inv %*% matrix(mu_c, ncol=30, nrow=2, byrow=FALSE))
  
  # Draw samples for all 30 groups at once (each row is a rat)
  new_params <- t(rmvnorm(n = 30, mean = rep(0, 2), sigma = Di)) + means  # Ensuring correct shape
  
  # Extract alpha and beta
  alphas[i, ] <- alpha <- new_params[1, ]  # First row is alpha
  betas[i, ] <- beta <- new_params[2, ]   # Second row is beta

  
  # --- 2. Sample mu | theta, tau ---
  ## Update the group parameters
  theta_bar = matrix(c(mean(alpha), mean(beta)), nrow=2)
  mu_c = rmvnorm(n = 1, 
                 mean = V %*% (30 * Sigma_inv %*% theta_bar + C_inv %*% eta),
                 sigma = V)
  mu_c = as.vector(mu_c)
  mu_cs[i,] = mu_c

  
  # --- 3. Sample tau | theta ---
  # Compute SSE efficiently
  residuals <- Y_matrix - Xi %*% new_params
  sse <- sum(residuals^2)
  sses[i] <- sse  # Store SSE
  
  # Update tau
  tau <- rinvgamma(n=1,
                   shape = (nu0 + 150)/ 2,
                   scale = (1/2) * (nu0 * lambda0 + sse))
  taus[i] <- tau
}



# POSTERIOR INFERENCE

# Remove burn-in
alpha_post <- alphas[(burn_in+1):n_iter, ]
beta_post <- betas[(burn_in+1):n_iter, ]
alpha_c_post <- mu_cs[(burn_in+1):n_iter, 1]
beta_c_post <- mu_cs[(burn_in+1):n_iter, 2]
tau_post <- taus[(burn_in+1):n_iter]

# Thin
alpha_post <- alpha_post[seq(1, nrow(alpha_post), by=thin), ]
beta_post <- beta_post[seq(1, nrow(beta_post), by=thin), ]
alpha_c_post <- alpha_c_post[seq(1, length(alpha_c_post), by=thin)]
beta_c_post <- beta_c_post[seq(1, length(beta_c_post), by=thin)]
tau_post <- tau_post[seq(1, length(tau_post), by=thin)]

# Compute posterior means, variances, and credible intervals
alpha_est <- apply(alpha_post, 2, mean)
alpha_var <- apply(alpha_post, 2, var)
alpha_ci <- apply(alpha_post, 2, function(x) quantile(x, probs = c(0.025, 0.975)))

beta_est <- apply(beta_post, 2, mean)
beta_var <- apply(beta_post, 2, mean)
beta_ci <- apply(beta_post, 2, function(x) quantile(x, probs = c(0.025, 0.975)))

alpha_c_est <- mean(alpha_c_post)
alpha_c_var <- var(alpha_c_post)
alpha_c_ci <- quantile(alpha_c_post, probs = c(0.025, 0.975))
beta_c_est <- mean(beta_c_post)
beta_c_var <- var(beta_c_post)
beta_c_ci <- quantile(beta_c_post, probs = c(0.025, 0.975))

tau_est <- mean(tau_post)
tau_var <- var(tau_post)
tau_ci <- quantile(tau_post, probs = c(0.025, 0.975))

# Print posterior estimates
cat("Posterior Mean of alpha:\n", alpha_est, "\n")
cat("Posterior Variance of alpha:\n", alpha_var, "\n")
cat("95% CI for alpha:\n", alpha_ci, "\n\n")

cat("Posterior Mean of beta:\n", beta_est, "\n")
cat("Posterior Variance of beta:\n", beta_var, "\n")
cat("95% CI for beta:\n", beta_ci, "\n\n")

cat("Posterior Mean of alpha_c:\n", alpha_c_est, "\n")
cat("Posterior Variance of alpha_c:\n", alpha_c_var, "\n")
cat("95% CI for alpha_c:\n", alpha_c_ci, "\n\n")

cat("Posterior Mean of beta_c:\n", beta_c_est, "\n")
cat("Posterior Variance of beta_c:\n", beta_c_var, "\n")
cat("95% CI for beta_c:\n", beta_c_ci, "\n\n")

cat("Posterior Mean of tau:\n", tau_est, "\n")
cat("Posterior Variance of tau:\n", tau_var, "\n")
cat("95% CI for tau:\n", tau_ci, "\n")


# Create a data frame to store the results
ci_table <- data.frame(
  Alpha_Mean = alpha_est,           # Posterior mean of alpha
  Alpha_Lower = alpha_ci[1, ],    # Lower bound of alpha CI
  Alpha_Upper = alpha_ci[2, ],    # Upper bound of alpha CI
  Beta_Mean = beta_est,             # Posterior mean of beta
  Beta_Lower = beta_ci[1, ],      # Lower bound of beta CI
  Beta_Upper = beta_ci[2, ]       # Upper bound of beta CI
)

# View the table
print(ci_table)




# Covariance matrix of alpha_c, beta_c, tau
joint <- cbind(alpha_c_post, beta_c_post, tau_post)
cov(joint)
```

```{r}
# CONVERGENCE GRAPHICAL DIAGNOSTICS

# Convert to mcmc objects
mcmc_alpha <- mcmc(alpha_post)
mcmc_beta <- mcmc(beta_post)
mcmc_alpha_c <- mcmc(alpha_c_post)
mcmc_beta_c <- mcmc(beta_c_post)
mcmc_tau <- mcmc(tau_post)

# Trace plots
par(mfrow = c(1,1))
traceplot(mcmc_alpha_c, main = "Traceplot of alpha_c")
traceplot(mcmc_beta_c, main = "Traceplot of beta_c")
traceplot(mcmc_tau, main = "Traceplot of tau")

# Trace plots for alpha
png("alpha_trace_plots.png", width=1500, height=1000)
par(mfrow=c(6,5))
for (i in 1:30) {
  # Plot trace for alpha (i-th observation)
  plot(1:length(alpha_post[,1]), alpha_post[, i], type = "l", 
       main = paste("Trace plot for Alpha", i),
       xlab = "Iteration", ylab = "Alpha Value", col = "black")
}
dev.off()

# Trace plots for beta
png("beta_trace_plots.png", width=1500, height=1000)
par(mfrow=c(6,5))
for (i in 1:30) {
  # Plot trace for beta (i-th observation)
  plot(1:length(beta_post[,1]), beta_post[, i], type = "l", 
       main = paste("Trace plot for Beta", i),
       xlab = "Iteration", ylab = "Beta Value", col = "black")
}

dev.off()


# Autocorrelation
par(mfrow=c(1,1))
acf(mcmc_tau, main = "ACF of tau")
acf(mcmc_alpha_c, main = "ACF of alpha_c")
acf(mcmc_beta_c, main = "ACF of beta_c")

# ACF plots for alpha
png("alpha_acf_plots.png", width=1500, height=1000)
par(mfrow=c(6,5))
for (i in 1:30) {
  # ACF for alpha (i-th observation)
  acf(alpha_post[, i], main = paste("ACF for Alpha", i))
}
dev.off()

# ACF plots for beta
png("beta_acf_plots.png", width=1500, height=1000)
par(mfrow=c(6,5))
for (i in 1:30) {
  # ACF for beta (i-th observation)
  acf(beta_post[, i], main = paste("ACF for Beta", i))
}
dev.off()

# Posterior densities
par(mfrow=c(1,1))
densplot(mcmc_alpha_c, main = "Posterior density of alpha_c")
densplot(mcmc_beta_c, main = "Posterior density of beta_c")
densplot(mcmc_tau, main = "Posterior density of tau")

# Posterior densities for alpha
png("alpha_density_plots.png", width=1500, height=1000)
par(mfrow=c(6,5))
for (i in 1:30) {
  # Density Plot for alpha (i-th observation)
  densplot(mcmc_alpha[, i], main = paste("Density Plot for Alpha", i))
}
dev.off()

# Posterior densities for beta
png("beta_density_plots.png", width=1500, height=1000)
par(mfrow=c(6,5))
for (i in 1:30) {
  # Density Plot for beta (i-th observation)
  densplot(mcmc_beta[, i], main = paste("Density Plot for Beta", i))
}
dev.off()
```

```{r}
# Geweke Tests for each parameter

geweke_results_alpha <- apply(alpha_post, 2, function(chain) {
  geweke.diag(mcmc(chain))$z
})

geweke_results_beta <- apply(beta_post, 2, function(chain) {
  geweke.diag(mcmc(chain))$z
})

geweke_results_alpha_c <- geweke.diag(mcmc_alpha_c)$z

geweke_results_beta_c <- geweke.diag(mcmc_beta_c)$z

geweke_results_tau <- geweke.diag(mcmc_tau)$z

print(list(geweke_results_alpha, geweke_results_beta, geweke_results_alpha_c, geweke_results_beta_c, geweke_results_tau))
```

None of them are far away enough from 0 to worry me too much, so I'm happy to conclude that all these have done a good job of converging!




